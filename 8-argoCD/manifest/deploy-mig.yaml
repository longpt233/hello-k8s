apiVersion: v1
kind: Pod
metadata:
  name: test-1
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia # k3s setitng requirement
  containers:
  - name: gpu-test
    # image: nvcr.io/nvidia/pytorch:24.01-py3  # cai nay nagn vl
    image: nvidia/cuda:12.2.0-base-ubuntu22.04
    command: [ "nvidia-smi" ]
    args: ["-L" ]
    resources:
      limits:
        nvidia.com/gpu: 1
  nodeSelector:
      nvidia.com/gpu.product : NVIDIA-A30-MIG-2g.12gb

---
apiVersion: v1
kind: Pod
metadata:
  name: test-2
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: gpu-test
    image: nvidia/cuda:12.2.0-base-ubuntu22.04
    command: [ "nvidia-smi" ]
    args: ["-L" ]
    resources:
      limits:
        nvidia.com/gpu: 1
  nodeSelector:
      nvidia.com/gpu.product : NVIDIA-A30-MIG-2g.12gb
      # https://www.scaleway.com/en/docs/compute/gpu/how-to/use-mig-with-kubernetes/